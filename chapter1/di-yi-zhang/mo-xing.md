# 文本表示

---

one-hot的表示方式，正交，两两之间的距离都是0，稀疏的向量

# 分布式表示

---

每个表示成为向量的形式

不是稀疏的向量而是稠密的向量

利用更小的维度表示更多的单词

# 构建目标函数（可出详解）

---

因为两个单词挨在一起，那么我们希望概率越大越好，变为一个类似的二分类的问题

负样本：不挨在一起的都是负样本，如果把所有的都考虑进来，那么负样本是特别多的。对于每一个正样本考虑部分的负样本

建立好负样本：

当前词作为中心词还是上下文词，所使用的向量是不一样的。这也是为什么使用两种向量的原因。



# Skip-Gram模型与知识图谱的结合

---

graph embedding

把每个节点和边表示成向量

# 矩阵分解

---

把V\*V的矩阵进行分解



# 高斯嵌入模型 Guassian Embedding

---

有些单词出现的很多，有些单词出现的很少，confidence 

词库中的单词分布：词频的多与少严格影响最终结果

表示两个分布的相似度：KL散度































